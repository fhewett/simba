{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Simba model\n",
    "\n",
    "Notebook to detail steps of training/fine-tuning/inference of the model used for Simba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/freya/simplification/simp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/freya/simplification/simp_env/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='train_inst_dataset.jsonl')\n",
    "dataset = dataset['train']\n",
    "dataset.set_format(\"torch\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training datapoint example**\n",
    "\n",
    "{\"original\": \"Kannst du folgenden Zeitungsartikel vereinfachen: 'Ein Netzwerkfehler sorgte Dienstagvormittag für den Ausfall von 120 der 275 Lifte in den Wiener U-Bahn-Stationen. Die Störung sei in der Nacht auf Dienstag um 3.47 Uhr eingetreten, berichtete ORF Wien . Durch den Netzwerkfehler ist wenig später in den betroffenen Liften die Notruffunktion ausgefallen. Aus Sicherheitsgründen habe man die Aufzüge gestoppt, sagte eine Sprecherin der Wiener Linien. Nur jene Stationen, die mit Aufsichtspersonal besetzt sind, seien nicht betroffen, da dort keine Gefahr bestehe, dass Personen unbemerkt stecken blieben bzw. zu Schaden kämen, hieß es. Nach der Reparatur des Systems wurden die Aufzüge schrittweise wieder in Gang gesetzt. Um 10.30 Uhr waren laut Wiener Linien alle ausgefallenen Aufzüge wieder in Betrieb. Was zu dem Defekt führte, ist derzeit noch Gegenstand von Untersuchungen. Für die Fahrgäste war die Sperre zwar ärgerlich, besondere Vorfälle gab es aber nicht, bestätigte auch die Wiener Rettung auf Nachfrage des KURIER. Der nächste Ausfall ist geplant: Ab 30. April wird die U4 von Hütteldorf bis Hietzing gesperrt .'\", \"simplification\": \"In den Wiener U-Bahn-Stationen ist von 18. auf 19. April ein technischer-Fehler aufgetreten. Durch diesen Fehler sind in den betroffenen Aufzügen die Notrufsignale ausgefallen. Aus Sicherheitsgründen waren viele Aufzüge außer Betrieb. Das war in Stationen, wo es keine Aufsichts-Person gab. In den Stationen, wo es eine Aufsicht gibt, wurden keine Aufzüge gestoppt, da keine Gefahr für die Menschen war. Nach der Reparatur von dem technischen Problem wurden schrittweise die Aufzüge wieder in Betrieb genommen. Was der Auslöser für diesen Fehler war, wird noch untersucht. Für die Menschen war die Sperre zwar ärgerlich, aber zum Glück gab es keine Verletzten.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"jphme/em_german_leo_mistral\"\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    #quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    #trust_remote_code=True,\n",
    "    #use_auth_token=True\n",
    ")\n",
    "#base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/freya/simplification/simp_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./mistral_fine_tuned_feb24\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=output_dir[2:], learning_rate=3e-5, warmup_steps=20, lr_scheduler_type=\"cosine\", adam_beta1=0.9, adam_beta2=0.95)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "#base_model.config.pretraining_tp = 1 \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)#, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    logging_steps=10,\n",
    "    max_steps=500\n",
    "    #num_train_epochs=1\n",
    ")\n",
    "\n",
    "#(output_dir=\"mistral_fine_tune_1011\", learning_rate=3e-5, warmup_steps=20, lr_scheduler_type=\"cosine\", adam_beta1=0.9, adam_beta2=0.95)\n",
    "#max_seq_length = 512\n",
    "\n",
    "#Du bist ein hilfreicher Assistent. USER: <instruction> ASSISTANT:\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    #print(example)\n",
    "    output_texts = []\n",
    "    for i in range(len(example['original'])):\n",
    "        text = f\"Du bist ein hilfreicher Assistent. USER: {example['original'][i]} ASSISTANT: {example['simplification'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    #max_seq_length=\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=training_args,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 16:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.291200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.2665534152984619, metrics={'train_runtime': 1008.9171, 'train_samples_per_second': 1.982, 'train_steps_per_second': 0.496, 'total_flos': 7.254548169488794e+16, 'train_loss': 1.2665534152984619, 'epoch': 0.21})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "#import torch\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "adapter_model_name=\"/home/freya/simplification/mistral_fine_tuned_feb24/final_checkpoint/\"\n",
    "base_model_name = \"jphme/em_german_leo_mistral\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)#, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)#, #device_map=\"auto\")\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half()\n",
    "model.save_pretrained(\"merged_model_mistral_fine_tuned_feb24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HF\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"merged_model_mistral_fine_tuned_feb24\",\n",
    "    repo_id=\"hiig-piai/simba-v01b_merged\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = \"Du bist ein hilfreicher Assistent. USER: Kannst du folgenden Zeitungsartikel vereinfachen: '\" \n",
    "prompt_format2 = \"' ASSISTANT: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = \"\"\"Schulen, Kitas, Bürgerämter: Am Mittwoch dürften viele Berlinerinnen und Berliner Einschränkungen bemerken - wegen eines Warnstreiks der Beschäftigten des öffentlichen Dienstes. Gewerkschaften fordern 10,5 Prozent mehr Lohn.\n",
    "\n",
    "Notbetreuung in Schulen, geschlossene Kitas, eingeschränkte Besetzung der Bürgerämter: Am Mittwoch hat ein Warnstreik der Berliner Beschäftigten, die unter den Tarifvertrag der Länder fallen, begonnen. Es sei mit Unterrichtsausfall an zahlreichen Schulen zu rechnen, sagte ein Sprecher der beteiligten Gewerkschaft Erziehung und Wissenschaft (GEW) am Morgen. Mindestens 100 Kitas blieben geschlossen, sagte ein Verdi-Sprecher dem rbb.\n",
    "\n",
    "Zum Warnstreik aufgerufen sind die Beschäftigten der Senatsverwaltungen und Bezirksämter, die Schulen und Hochschulen, die Polizeidienststellen, die Feuerwehr, die Kitas und die forstwirtschaftlichen Betriebe des Landes Berlin, wie die Gewerkschaften Verdi, Erziehung und Wissenschaft (GEW), der Polizei (GdP) und IG Bau gemeinsam mitteilten.\n",
    "\n",
    "Begleitet wird der Streik in Berlin von einer Demo am Vormittag. Sie beginnt am Wittenbergplatz und zieht dann bis zum Platz des 18. März. Die Organisatoren rechnen eigenen Angaben zufolge mit Tausenden Teilnehmern. Auch in den anderen Stadtstaaten Bremen und Hamburg sind die Beschäftigten des öffentlichen Diensts zu Arbeitsniederlegungen aufgerufen.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning output (this might be able to be done more cleverly, i.e. maybe there's some model params that do the same thing?)\n",
    "# ensure output is not cut off mid-sentence\n",
    "# ensure output doesn't have repetitive content\n",
    "\n",
    "import re\n",
    "\n",
    "def split_into_sentences(text):\n",
    "  # from here: https://github.com/brjezierski/scrapers/blob/51da6fa87879217c5676df87a5f28873ee8e0826/preprocess.py#L88C1-L100C19 \n",
    " \n",
    "  sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![0-9]\\.)(?<![0-9][0-9]\\.)(?<![A-Z]\\.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", text)\n",
    "  sentences = [s for s in sentences if s]\n",
    "  return sentences\n",
    "\n",
    "from difflib import SequenceMatcher as SM\n",
    "\n",
    "def remove_repetitive_output(raw_output):\n",
    "\n",
    "    output_sents = split_into_sentences(raw_output.split(\"ASSISTANT: \")[1])\n",
    "\n",
    "    outputs = \"\"\n",
    "    sentences = list()\n",
    "\n",
    "    for x in range(1,len(output_sents)):\n",
    "        if SM(None,output_sents[x-1],output_sents[x]).ratio() < 0.6:\n",
    "            if output_sents[x-1][-1] == \".\":\n",
    "                if output_sents[x-1].replace(\"\\n\", \"\") not in sentences:\n",
    "                    outputs += output_sents[x-1].replace(\"\\n\", \"\") + \" \"\n",
    "                    sentences.append(output_sents[x-1].replace(\"\\n\", \"\"))\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model if not already loaded\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model=\"merged_model_mistral_fine_tuned_feb24\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "base_model_name = \"jphme/em_german_leo_mistral\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)#, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Am Mittwoch haben in Berlin viele Beschäftigte des öffentlichen Dienstes gestreikt. Das bedeutet, dass sie nicht gearbeitet haben. Die Beschäftigten haben gestreikt, weil sie mehr Geld bekommen wollen. Sie fordern 10,5 Prozent mehr Lohn. Das bedeutet, dass sie 10,5 Prozent mehr Geld bekommen sollen. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = tokenizer(prompt_format + text5 + prompt_format2, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "remove_repetitive_output(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration dataset (unseen training examples)\n",
    "\n",
    "import pickle\n",
    "formatted_calibrate_ds = pickle.load(open(\"calibrated_ds.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "import torch\n",
    "\n",
    "fine_tuned_model = \"merged_model_mistral_fine_tuned_feb24\"\n",
    "base_model_name = \"jphme/em_german_leo_mistral\"\n",
    "\n",
    "#Load model on CPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model, torch_dtype=torch.float16)#.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing model.layers blocks : 100%|██████████| 32/32 [18:13<00:00, 34.19s/it]\n",
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
     ]
    }
   ],
   "source": [
    "quantizer = GPTQQuantizer(bits=4, group_size=128, dataset=formatted_calibrate_ds[:150], cache_examples_on_gpu=False)#, block_name_to_quantize = \"model.decoder.layers\", model_seqlen = 2048)\n",
    "quantized_model = quantizer.quantize_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_model_mistral_fine_tuned_feb24-gptq/tokenizer_config.json',\n",
       " 'merged_model_mistral_fine_tuned_feb24-gptq/special_tokens_map.json',\n",
       " 'merged_model_mistral_fine_tuned_feb24-gptq/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(fine_tuned_model+\"-gptq\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(fine_tuned_model+\"-gptq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 4.16G/4.16G [03:33<00:00, 19.5MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hiig-piai/simba-v01b_merged_gptq/commit/4613d8656c36a8bb3899d8d68c7b0545f76c771f', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4613d8656c36a8bb3899d8d68c7b0545f76c771f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to HF\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=fine_tuned_model+\"-gptq\",\n",
    "    repo_id=\"hiig-piai/simba-v01b_merged_gptq\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
