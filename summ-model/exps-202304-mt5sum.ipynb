{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook to experiment with the MT5 TSystems' abstractive summarizer model\n",
    "\n",
    "Author: Hadi, Ver 202304\n",
    "\n",
    "I am interested in two things: Speed and Quality. Quality itself has three aspects, (i) not fibbing (ii) not cutting off (iii) sufficient info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apple opens first flagship store in India\n",
      "\n",
      "22 hours ago22 hours ago\n",
      "\n",
      "Hundreds of people waited in line outside the new Apple store in the financial\n",
      "hub Mumbai. The company is planning to boost its production in India — the\n",
      "second-largest smartphone market in the world.\n",
      "\n",
      "https://p.dw.com/p/4QDnu\n",
      "\n",
      "Advertisement\n",
      "\n",
      "Apple CEO Tim Cook was in Mumbai on Tuesday to inaugurate the company's first\n",
      "retail store in India.\n",
      "\n",
      "\"India has such a beautiful culture and an incredible energy, and we're\n",
      "excited to build on our long-standing history,\" Cook said in a statement.\n",
      "\n",
      "Bloggers, tech analysts and Bollywood celebrities were invited to the opening\n",
      "of the new store in Mumbai's Jio World Drive shopping mall.\n",
      "\n",
      "About 300 Apple fans were seen lining up outside the store, some of them\n",
      "waiting since the previous night.\n",
      "\n",
      "\"The fanboy inside me would not listen,\" 30-year-old Purav Mehta, who wanted\n",
      "to get Cook's autograph, told Reuters news agency.\n",
      "\n",
      "> A second store is set to open in the capital, New Delhi, on Thursday. Cook\n",
      "> is also expected to meet Indian Prime Minister Narendra Modi and India's\n",
      "> deputy IT minister later this week.\n",
      "## Eye on the Indian market\n",
      "\n",
      "The bulk of Apple's smartphone and tablets are assembled in China, but the\n",
      "tech giant also has plans to expand production in India.\n",
      "\n",
      "The South Asian country of 1.4 billion people is also the second-largest\n",
      "smartphone market in the world.\n",
      "\n",
      "Apple has been operating in India for more than 25 years, and it began\n",
      "manufacturing products there in 2017.\n",
      "\n",
      "According to Counterpoint Research, India produces around 13 million iPhones\n",
      "every year, up from under 5 million three years ago.\n",
      "\n",
      "The tech giant has been selling its products in India online and through\n",
      "authorized retailers, but regulatory clearances and the pandemic had delayed\n",
      "its plan to open a flagship store in the country.\n",
      "\n",
      "Indian Commerce Minister Piyush Goyal said last week that Apple plans to have\n",
      "25% of their global production come out of India in the next five years.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOAD A WEBPAGE FOR ANALYSIS. Extracts pages content as text into `txt` variable\n",
    "\n",
    "import requests\n",
    "import html2text  # pip\n",
    "import lxml.html\n",
    "\n",
    "#url = \"https://www.dw.com/en/brazil-welcomes-russias-lavrov-amid-us-criticism/a-65353778\"\n",
    "url = \"https://www.dw.com/en/apple-opens-first-flagship-store-in-india/a-65354938\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "tree = lxml.html.fromstring(response.text)\n",
    "el = tree.xpath('//main|//div[@role=\"main\"]|//section[@role=\"main\"]|//div[@class=\"main-row\"]|//div[@class=\"main\"]|//div[@id=\"main\"]')\n",
    "if el:\n",
    "    src = \"\\n\".join(el.getall())\n",
    "    print(\"found\")\n",
    "else:\n",
    "    src = response.text\n",
    "    #print(\"not found\")\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "h.ignore_emphasis = True\n",
    "h.images_to_alt = False\n",
    "h.single_line_break = True\n",
    "txt = h.handle(src)\n",
    "\n",
    "# let's remove the fluf manually on this page\n",
    "if \"# Brazil\" in txt:\n",
    "    txt = txt[txt.find(\"# Brazil welcomes Russia's\"):txt.find(\"mf/wd (AP, AFP)\")]\n",
    "elif \"# Apple\" in txt:\n",
    "    txt = txt[txt.find(\"# Apple opens first\"):txt.find(\"mf/nm (AP, Reuters)\")]\n",
    "print(txt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T09:26:23.118621Z",
     "end_time": "2023-04-19T09:26:23.541486Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991 52\n"
     ]
    }
   ],
   "source": [
    "# TODO:  use a method to detect the text language, also fast\n",
    "\n",
    "print(len(txt), len(txt.split(\"\\n\")))  # 3121 chars, 80 lines, xx tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T09:26:23.541242Z",
     "end_time": "2023-04-19T09:26:23.545620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 379 ms, sys: 7.54 ms, total: 387 ms\n",
      "Wall time: 547 ms\n",
      "CPU times: user 3.86 s, sys: 629 ms, total: 4.49 s\n",
      "Wall time: 3.34 s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel = MT5ForConditionalGeneration.from_pretrained(\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT-Systems-onsite/mt5-small-sum-de-en-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)  # 4.3s, once\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#model.half()  # 92ms, speeds up inference, mem at 708MiB\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 99ms\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# model.eval() unnecessary\u001B[39;00m\n\u001B[1;32m      8\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnvidia-smi\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.venv/simba2/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    889\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    890\u001B[0m \n\u001B[1;32m    891\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 905\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/simba2/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/simba2/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/.venv/simba2/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    889\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    890\u001B[0m \n\u001B[1;32m    891\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 905\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.venv/simba2/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDA_MODULE_LOADING\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n\u001B[1;32m    246\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDA_MODULE_LOADING\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLAZY\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 247\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# we need to just return without initializing in that case.\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# However, we must not let any *other* threads in!\u001B[39;00m\n\u001B[1;32m    251\u001B[0m _tls\u001B[38;5;241m.\u001B[39mis_initializing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "%time tokenizer = MT5Tokenizer.from_pretrained(\"T-Systems-onsite/mt5-small-sum-de-en-v2\")  # 0.5s, once\n",
    "%time model = MT5ForConditionalGeneration.from_pretrained(\"T-Systems-onsite/mt5-small-sum-de-en-v2\")  # 4.3s, once\n",
    "#model.half()  # 92ms, speeds up inference, mem at 708MiB\n",
    "model.cuda()  # 99ms\n",
    "# model.eval() unnecessary\n",
    "\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T22:18:19.327448Z",
     "end_time": "2023-04-18T22:18:23.923344Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 201\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/356 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4aab6a0488504d6bb87160c8034305eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a4cc3ba25674d8097096ee99c5b0169"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a07c4c88149f48e49db379279428c1bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c81ef22dca164244b299660027bc3fa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99b3f5a4964546e88e971842ae99afbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER RELATED QUESTIONS/INVESTIGATIONS.\n",
    "# - use a fast tokenizer to get number of tokens (should be >20tokz, <512tokz) → resolved, below\n",
    "# - actualy best method is to use the tokenizer from the models themselves\n",
    "\n",
    "# import spacy\n",
    "# SPACY_NLP = spacy.load(\"de_core_news_sm\")\n",
    "# s = list(SPACY_NLP(txt).sents)[0]\n",
    "# print(len(s), len(str(s)))\n",
    "#\n",
    "# from transformers import AutoTokenizer\n",
    "# tokz = AutoTokenizer.from_pretrained(('symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli'))\n",
    "# print(tokz.model_max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T01:04:06.322214Z",
     "end_time": "2023-04-19T01:04:15.848341Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:1\u001B[0m\n",
      "File \u001B[0;32m~/.venv/simba2/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDA_MODULE_LOADING\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n\u001B[1;32m    246\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDA_MODULE_LOADING\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLAZY\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 247\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# we need to just return without initializing in that case.\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# However, we must not let any *other* threads in!\u001B[39;00m\n\u001B[1;32m    251\u001B[0m _tls\u001B[38;5;241m.\u001B[39mis_initializing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# 625ms on GPU with or without beams\u001B[39;00m\n\u001B[1;32m      6\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput = model.generate(input_ids=input_ids.cuda(), max_new_tokens=120, repetition_penalty=1.5)  # do_sample=True, num_beams=4)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(\u001B[43moutput\u001B[49m[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# mmm, max_new_tokens=20 does good job. without it not so much.\n",
    "\n",
    "input_ids = tokenizer.encode(txt, return_tensors=\"pt\", max_length=512, truncation=True)  # 3ms\n",
    "\n",
    "# 625ms on GPU with or without beams\n",
    "%time output = model.generate(input_ids=input_ids.cuda(), max_new_tokens=120, repetition_penalty=1.5)  # do_sample=True, num_beams=4)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/.venv/simba2/lib/python3.10/site-packages/optimum/intel/neural_compressor/quantization.py:657: UserWarning: The class `IncQuantizer` has been depreciated and will be removed in optimum-intel v1.7, please use `INCQuantizer` instead.\n",
      "  warnings.warn(\n",
      "/home/hadi/.venv/simba2/lib/python3.10/site-packages/optimum/intel/neural_compressor/trainer.py:817: UserWarning: The class `IncTrainer` has been depreciated and will be removed in optimum-intel v1.7, please use `INCTrainer` instead.\n",
      "  warnings.warn(\n",
      "2023-04-19 09:33:12 [WARNING] Force convert framework model to neural_compressor model.\n",
      "2023-04-19 09:33:12 [INFO] Start auto tuning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 s, sys: 473 ms, total: 4.53 s\n",
      "Wall time: 3.32 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 09:33:12 [INFO] Pass query framework capability elapsed time: 766.73 ms\n",
      "2023-04-19 09:33:12 [INFO] Adaptor has 1 recipes.\n",
      "2023-04-19 09:33:12 [INFO] 0 recipes specified by user.\n",
      "2023-04-19 09:33:12 [INFO] 0 recipes require future tuning.\n",
      "2023-04-19 09:33:12 [INFO] *** Initialize auto tuning\n",
      "2023-04-19 09:33:12 [INFO] Get FP32 model baseline.\n",
      "2023-04-19 09:33:12 [INFO] Save tuning history to /home/hadi/Source/simba/summ-model/nc_workspace/2023-04-19_09-31-26/./history.snapshot.\n",
      "2023-04-19 09:33:12 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2023-04-19 09:33:12 [INFO] Quantize the model with default config.\n",
      "2023-04-19 09:33:13 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n",
      "2023-04-19 09:33:20 [INFO] |******Mixed Precision Statistics******|\n",
      "2023-04-19 09:33:20 [INFO] +-----------------+----------+---------+\n",
      "2023-04-19 09:33:20 [INFO] |     Op Type     |  Total   |   INT8  |\n",
      "2023-04-19 09:33:20 [INFO] +-----------------+----------+---------+\n",
      "2023-04-19 09:33:20 [INFO] |    Embedding    |    3     |    3    |\n",
      "2023-04-19 09:33:20 [INFO] |      Linear     |   145    |   145   |\n",
      "2023-04-19 09:33:20 [INFO] +-----------------+----------+---------+\n",
      "2023-04-19 09:33:20 [INFO] Pass quantize model elapsed time: 7571.33 ms\n",
      "2023-04-19 09:33:20 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2023-04-19 09:33:20 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2023-04-19 09:33:20 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2023-04-19 09:33:20 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2023-04-19 09:33:20 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2023-04-19 09:33:20 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n",
      "2023-04-19 09:33:20 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n",
      "2023-04-19 09:33:20 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2023-04-19 09:33:20 [INFO] Save tuning history to /home/hadi/Source/simba/summ-model/nc_workspace/2023-04-19_09-31-26/./history.snapshot.\n",
      "2023-04-19 09:33:20 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2023-04-19 09:33:20 [INFO] Save deploy yaml to /home/hadi/Source/simba/summ-model/nc_workspace/2023-04-19_09-31-26/deploy.yaml\n",
      "Model weights saved to dynamic_quantization_mt5sum/pytorch_model.bin\n",
      "Configuration saved in dynamic_quantization_mt5sum/inc_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 1.36 s, total: 21.5 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "# INTEL int8 quantization\n",
    "#!pip install onnx neural-compressor git+https://github.com/huggingface/optimum-intel.git\n",
    "\n",
    "from optimum.intel import INCQuantizer\n",
    "from optimum.intel import INCModelForSeq2SeqLM\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "# Load model to quantize. Keep it on cpu and FP32.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "%time model = MT5ForConditionalGeneration.from_pretrained(\"T-Systems-onsite/mt5-small-sum-de-en-v2\")  # 4.3s\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"T-Systems-onsite/mt5-small-sum-de-en-v2\")  # 1s, once\n",
    "\n",
    "# Apply dynamic quantization and save the resulting model\n",
    "quantizer = INCQuantizer.from_pretrained(model)\n",
    "%time quantizer.quantize(quantization_config=PostTrainingQuantConfig(approach=\"dynamic\"), save_directory=\"iquant_mt5sum\")  # 20s\n",
    "\n",
    "\n",
    "# lets reload it\n",
    "%time model_iq = INCModelForSeq2SeqLM.from_pretrained(\"iquant_mt5sum\")  # 15-25s!\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T09:33:08.641547Z",
     "end_time": "2023-04-19T09:33:22.650300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem quant: 488.55859375\n",
      "<pad> Apple CEO Tim Cook is in Mumbai to inaugurate the company's first store in India. The company is planning to boost its production in India. The South Asian country of 1.4 billion people is also the second-largest smartphone market in the world.</s>\n",
      "CPU times: user 8.12 s, sys: 8.07 ms, total: 8.13 s\n",
      "Wall time: 2.36 s\n",
      "\n",
      "mem regul: 1145.03662109375\n",
      "<pad> Apple CEO Tim Cook is in Mumbai to inaugurate the company's first retail store in India. The company is planning to boost its production in India. The company is planning to boost its production in India.</s>\n",
      "CPU times: user 9.6 s, sys: 0 ns, total: 9.6 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "# speed tests\n",
    "print(\"mem quant:\", model_iq.get_memory_footprint()/(1024*1024))   # 488MB\n",
    "%time print(tokenizer.decode(model_iq.generate(input_ids, max_new_tokens=120, do_sample=False)[0]))\n",
    "\n",
    "print()\n",
    "print(\"mem regul:\", model.get_memory_footprint()/(1024*1024))  # 1145MB\n",
    "%time print(tokenizer.decode(model.generate(input_ids, max_new_tokens=120, do_sample=False)[0]))\n",
    "\n",
    "# => HUH, IT SEEMS THE DYNAMIC QUANTIZED MODEL IS SLOWER\n",
    "#   facinating outputs are slightly different with same inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T10:36:51.332307Z",
     "end_time": "2023-04-19T10:36:54.894216Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
